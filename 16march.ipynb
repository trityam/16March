{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d57c68-741a-426c-a7e5-5dc2799b218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?\n",
    "\n",
    "__Underfitting in Machine Learning\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple \n",
    "to capture data complexities. It represents the inability of the model to learn the training data effectively\n",
    "result in poor performance both on the training and testing data\n",
    "\n",
    "__Techniques to Reduce Underfitting\n",
    "1)Increase model complexity.\n",
    "2)Increase the number of features, performing feature engineering.\n",
    "3)Remove noise from the data.\n",
    "4)Increase the number of epochs or increase the duration of training to get better results.\n",
    "\n",
    "__Overfitting in Machine Learning\n",
    "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data.\n",
    "When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in \n",
    "our data set. And when testing with test data results in High variance. Then the model does not categorize the \n",
    "data correctly, because of too many details and noise.\n",
    "\n",
    "__Techniques to Reduce Overfitting\n",
    "1)Improving the quality of training data reduces overfitting by focusing on meaningful patterns, mitigate the\n",
    "risk of fitting the noise or irrelevant features.\n",
    "2)Increase the training data can improve the model’s ability to generalize to unseen data and reduce the\n",
    "likelihood of overfitting.\n",
    "3)Reduce model complexity.\n",
    "4)Early stopping during the training phase (have an eye over the loss over the training period as soon as \n",
    "loss begins to increase stop training).\n",
    "5)Ridge Regularization and Lasso Regularization.\n",
    "6)Use dropout for neural networks to tackle overfitting.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434e534-15da-4e25-bfd9-669400cd7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q2: How can we reduce overfitting? Explain in brief.\n",
    "__Techniques to Reduce Overfitting\n",
    "1)Improving the quality of training data reduces overfitting by focusing on meaningful patterns, mitigate the\n",
    "risk of fitting the noise or irrelevant features.\n",
    "2)Increase the training data can improve the model’s ability to generalize to unseen data and reduce the\n",
    "likelihood of overfitting.\n",
    "3)Reduce model complexity.\n",
    "4)Early stopping during the training phase (have an eye over the loss over the training period as soon as \n",
    "loss begins to increase stop training).\n",
    "5)Ridge Regularization and Lasso Regularization.\n",
    "6)Use dropout for neural networks to tackle overfitting.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a928c-c620-4903-9345-6812f996dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "__Underfitting in Machine Learning\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple \n",
    "to capture data complexities. It represents the inability of the model to learn the training data effectively\n",
    "result in poor performance both on the training and testing data\n",
    "\n",
    "__Reasons for Underfitting\n",
    "1)The model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "2)The input features which is used to train the model is not the adequate representations of underlying factors\n",
    "influencing the target variable.\n",
    "3)The size of the training dataset used is not enough.\n",
    "4)Excessive regularization are used to prevent the overfitting, which constraint the model to capture the\n",
    "data well.\n",
    "5)Features are not scaled.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0276713-a800-4ca0-a42e-a6df44e29f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "\n",
    "__Bias-Variance Trade-Off\n",
    "While building the machine learning model, it is really important to take care of bias and variance in\n",
    "order to avoid overfitting and underfitting in the model. If the model is very simple with fewer parameters,\n",
    "it may have low variance and high bias. Whereas, if the model has a large number of parameters, it will have\n",
    "high variance and low bias. So, it is required to make a balance between bias and variance errors, and this \n",
    "balance between the bias error and variance error is known as the Bias-Variance trade-off.\n",
    "\n",
    "__Relationship between bias and varaince\n",
    "1)Low-Bias, Low-Variance:\n",
    "The combination of low bias and low variance shows an ideal machine learning model. However, it is not\n",
    "possible practically.\n",
    "2)Low-Bias, High-Variance:With low bias and high variance, model predictions are inconsistent and accurate\n",
    "on average. This case occurs when the model learns with a large number of parameters and hence leads to an\n",
    "overfitting\n",
    "3)High-Bias, Low-Variance: With High bias and low variance, predictions are consistent but inaccurate on\n",
    "average. This case occurs when a model does not learn well with the training dataset or uses few numbers\n",
    "of the parameter. It leads to underfitting problems in the model.\n",
    "4)High-Bias, High-Variance:\n",
    "With high bias and high variance, predictions are inconsistent and also inaccurate on average.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c07001-2bfc-4b1a-9674-c54b8ba5857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "We can identify if a machine learning model has overfit by first evaluating the model on the training \n",
    "dataset and then evaluating the same model on a holdout test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2d7c1-0595-428c-94bf-87e5900e267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "__Bias\n",
    "Every machine learning algorithm has a prediction error, which can be segmented into three subcomponents:\n",
    "bias error, variance error, and irreducible error. In the process of machine learning, faulty assumptions\n",
    "can lead to the occurrence of a phenomena known as bias.\n",
    "\n",
    "Bias can emerge in the model of machine learning. When an algorithm generates results that are systematically\n",
    "prejudiced due to some inaccurate assumptions that were made throughout the process of machine learning, this\n",
    "is an example of bias.\n",
    "\n",
    "Bias is analogous to a systematic error. They are presumptions that are made by a model in order to simplify\n",
    "the process of learning the target function.\n",
    "\n",
    "A high bias indicates that both the error in the training data and the error in the testing data are greater.\n",
    "To prevent the issue of underfitting, it is usually advised that an algorithm have a minimal bias in order to\n",
    "maximize accuracy.\n",
    "\n",
    "Let's imagine you've chosen a model that is incapable of deriving even the fundamental patterns from the data\n",
    "set; this is what we mean when we talk about underfitting. When you apply an algorithm to a problem and find\n",
    "that it does not fit adequately, you have a situation that might be characterized as biased.\n",
    "\n",
    "__Variance\n",
    "The difference in the accuracy of a machine learning model's predictions between the training data and the\n",
    "test data is referred to as variance. A variance error is what we call the situation when a change in the\n",
    "performance of the model is brought about by a variation in the dataset.\n",
    "\n",
    "Variance refers to the magnitude of the change that would occur in the estimation of the target function if \n",
    "a different set of training data was utilized. Because a machine learning algorithm infers the target \n",
    "function from the training data, it is reasonable to anticipate that the method will exhibit some degree \n",
    "of variability.\n",
    "\n",
    "Variance is dependent on a single training set, and it is the factor that determines the inconsistency of \n",
    "the predictions made using various training sets.\n",
    "\n",
    "When the variance is low, it suggests that the estimate of the target function will change only slightly\n",
    "when the training dataset is altered.\n",
    "\n",
    "When the variance is high, it suggests that the estimate of the target function will change significantly \n",
    "when the training dataset is altered.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a459e-a399-4195-8f17-8549933e0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.\n",
    "\n",
    "Regularization Technique\n",
    "Regularization is a technique in machine learning that helps prevent from overfitting. It works by introducing\n",
    "penalties term or constraints on the model’s parameters during training. These penalties term encourage the\n",
    "model to avoid extreme or overly complex parameter values. By doing so, regularization prevents the model from\n",
    "fitting the training data too closely, which is a common cause of overfitting. Instead, it promotes a balance\n",
    "between model complexity and performance, leading to better generalization on new, unseen data.\n",
    "\n",
    "How Regularization used to prevent overfitting\n",
    "By introducing the regularization term in loss function that act like a constrain function of the model’s \n",
    "parameter. This function penalize certain parameter values in model, discouraging them from becoming too large\n",
    "or complex.\n",
    "Regularization introduces a trade-off between fitting the training data and keeping the model’s parameters \n",
    "small. The strength of regularization is controlled by a hyperparameter, often denoted as lambda (λ). A higher λ value leads to stronger regularization and a simpler model.\n",
    "Regularization techniques help control the complexity of the model. They make the model more robust by constraining the parameter space. This results in smoother decision boundaries in the case of classification and smoother functions in regression, reducing the potential for overfitting.\n",
    "Regularization oppose overfitting by discouraging the model from fitting the training data too closely. It prevents parameters from taking extreme values, which might be necessary to fit the training data.\n",
    "Let’s discuss about two common techniques that involve in regularization which can prevent model from overfitting\n",
    "\n",
    "L1 Regularization\n",
    "L2 Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
